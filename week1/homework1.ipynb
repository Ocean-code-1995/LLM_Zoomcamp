{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***`Homework: Introduction`***\n",
    "---\n",
    "\n",
    "In this homework, we'll learn more about search and use Elastic Search for practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***`Q1. Running Elastic`***\n",
    "---\n",
    "\n",
    "Run Elastic Search 8.4.3, and get the cluster information. If you run it on localhost, this is how you do it:\n",
    "\n",
    "```bash\n",
    "curl localhost:9200\n",
    "```\n",
    "\n",
    "What's the version.build_hash value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First run the following below to get ElasticSearch up and running:\n",
    "\n",
    "```bash\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    --name elasticsearch \\\n",
    "    -p 9200:9200 \\\n",
    "    -p 9300:9300 \\\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    -e \"xpack.security.enabled=false\" \\\n",
    "    docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
    "```\n",
    "\n",
    "### *Why Use Two Ports: 9200 and 9300?*\n",
    "\n",
    "1. ***`Port 9200:`***\n",
    "\n",
    "- This is the default port for the HTTP REST API.\n",
    "#\n",
    "- It is used for communication between clients and Elasticsearch. When you use tools like `curl` or applications that interact with Elasticsearch, they typically communicate over this port.\n",
    "\n",
    "2. ***`Port 9300:`***\n",
    "\n",
    "- This is the default port for internal communication between Elasticsearch nodes.\n",
    "#\n",
    "- It is used for node-to-node communication within the Elasticsearch cluster. Even in a single-node setup, this port is necessary for certain internal processes and potential future cluster expansions.\n",
    "#\n",
    "\n",
    "### *What is a Cluster in Elasticsearch?*\n",
    "\n",
    "A **cluster** in Elasticsearch is a collection of one or more nodes (servers) that together store your entire data and provide federated indexing and search capabilities across all nodes.\n",
    "\n",
    "#### Key Concepts of an Elasticsearch Cluster:\n",
    "\n",
    "1. **Node**:\n",
    "   - A single server that is part of the cluster. Each node stores data and participates in the cluster's indexing and search capabilities.\n",
    "   - Nodes can join or leave the cluster dynamically.\n",
    "\n",
    "2. **Cluster**:\n",
    "   - A group of nodes with the same `cluster.name` setting, working together to share the workload.\n",
    "   - The cluster's health status and the distribution of tasks and data are managed collectively.\n",
    "\n",
    "3. **Cluster Name**:\n",
    "   - A unique name to identify a specific cluster. Nodes in the same cluster must have the same cluster name.\n",
    "   - The default name is \"elasticsearch\".\n",
    "\n",
    "4. **Master Node**:\n",
    "   - Manages the cluster by handling tasks such as creating/deleting indices, tracking which nodes are part of the cluster, and deciding where to allocate shards.\n",
    "   - Every cluster has one elected master node, but any node can become the master.\n",
    "\n",
    "5. **Data Node**:\n",
    "   - Stores the data and performs data-related operations like CRUD, search, and aggregations.\n",
    "   - In a multi-node cluster, you can have nodes dedicated solely to data handling.\n",
    "\n",
    "6. **Cluster State**:\n",
    "   - Maintained by the master node, it includes information about all the nodes, indices, and shards within the cluster.\n",
    "   - It ensures that all nodes have a consistent view of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"10b41fd5905e\",\n",
      "  \"cluster_name\" : \"docker-cluster\",\n",
      "  \"cluster_uuid\" : \"rEmcivvsQrOvQuQNzXH8Tw\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"8.4.3\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"docker\",\n",
      "    \"build_hash\" : \"42f05b9372a9a4a470db3b52817899b99a76ee73\",\n",
      "    \"build_date\" : \"2022-10-04T07:17:24.662462378Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"9.3.0\",\n",
      "    \"minimum_wire_compatibility_version\" : \"7.17.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"7.0.0\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl localhost:9200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: 42f05b9372a9a4a470db3b52817899b99a76ee73\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   539  100   539    0     0  35542      0 --:--:-- --:--:-- --:--:-- 35933\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Run the curl command and capture the output\n",
    "result = subprocess.run(['curl', 'localhost:9200'], stdout=subprocess.PIPE)\n",
    "response = result.stdout.decode('utf-8')\n",
    "\n",
    "# Parse the JSON response\n",
    "response_json = json.loads(response)\n",
    "\n",
    "# Extract the version.build_hash value\n",
    "build_hash = response_json['version']['build_hash']\n",
    "\n",
    "\n",
    "# answer\n",
    "print(f\"\\nAnswer: {build_hash}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***`Getting the data`***\n",
    "---\n",
    "\n",
    "Now let's get the FAQ data. You can run this snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'course': 'data-engineering-zoomcamp',\n",
      " 'question': 'Course - When will the course start?',\n",
      " 'section': 'General course-related questions',\n",
      " 'text': 'The purpose of this document is to capture frequently asked '\n",
      "         'technical questions\\n'\n",
      "         'The exact day and hour of the course will be 15th Jan 2024 at 17h00. '\n",
      "         \"The course will start with the first  “Office Hours'' live.1\\n\"\n",
      "         'Subscribe to course public Google Calendar (it works from Desktop '\n",
      "         'only).\\n'\n",
      "         'Register before the course starts using this link.\\n'\n",
      "         'Join the course Telegram channel with announcements.\\n'\n",
      "         \"Don’t forget to register in DataTalks.Club's Slack and join the \"\n",
      "         'channel.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "\n",
    "\n",
    "# see the first document\n",
    "pprint(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***`Q2. Indexing the data`***\n",
    "---\n",
    "\n",
    "Index the data in the same way as was shown in the course videos. Make the course field a keyword and the rest should be text.\n",
    "\n",
    "Which function do you use for adding your data to elastic?\n",
    "\n",
    "- insert\n",
    "- index\n",
    "- put\n",
    "- add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianwefers/Developer/miniconda/base/envs/llm_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "BadRequestError(400, 'resource_already_exists_exception', 'index [course-questions/XF0RN82rTge9LSwDzO7Arw] already exists')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m index_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcourse-questions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m es_client \u001b[38;5;241m=\u001b[39m Elasticsearch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://localhost:9200\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mes_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Index the documents\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm(documents):\n",
      "File \u001b[0;32m~/Developer/miniconda/base/envs/llm_env/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:446\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/miniconda/base/envs/llm_env/lib/python3.10/site-packages/elasticsearch/_sync/client/indices.py:553\u001b[0m, in \u001b[0;36mIndicesClient.create\u001b[0;34m(self, index, aliases, error_trace, filter_path, human, mappings, master_timeout, pretty, settings, timeout, wait_for_active_shards, body)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m __body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     __headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPUT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindices.create\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_parts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__path_parts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/miniconda/base/envs/llm_env/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:423\u001b[0m, in \u001b[0;36mNamespacedClient.perform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_request\u001b[39m(\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    412\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;66;03m# Use the internal clients .perform_request() implementation\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# so we take advantage of their transport options.\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_parts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_parts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/miniconda/base/envs/llm_env/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:271\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_request\u001b[39m(\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    257\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m     path_parts: Optional[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    265\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ApiResponse[Any]:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_otel\u001b[38;5;241m.\u001b[39mspan(\n\u001b[1;32m    267\u001b[0m         method,\n\u001b[1;32m    268\u001b[0m         endpoint_id\u001b[38;5;241m=\u001b[39mendpoint_id,\n\u001b[1;32m    269\u001b[0m         path_parts\u001b[38;5;241m=\u001b[39mpath_parts \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[1;32m    270\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m otel_span:\n\u001b[0;32m--> 271\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43motel_span\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43motel_span\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m         otel_span\u001b[38;5;241m.\u001b[39mset_elastic_cloud_metadata(response\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Developer/miniconda/base/envs/llm_env/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:352\u001b[0m, in \u001b[0;36mBaseClient._perform_request\u001b[0;34m(self, method, path, params, headers, body, otel_span)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTP_EXCEPTIONS\u001b[38;5;241m.\u001b[39mget(meta\u001b[38;5;241m.\u001b[39mstatus, ApiError)(\n\u001b[1;32m    353\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage, meta\u001b[38;5;241m=\u001b[39mmeta, body\u001b[38;5;241m=\u001b[39mresp_body\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# 'X-Elastic-Product: Elasticsearch' should be on every 2XX response.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verified_elasticsearch:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# If the header is set we mark the server as verified.\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: BadRequestError(400, 'resource_already_exists_exception', 'index [course-questions/XF0RN82rTge9LSwDzO7Arw] already exists')"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,    # single node (basic units of storage in Elasticsearch) -> can be distributed across multiple nodes\n",
    "        \"number_of_replicas\": 0   # replicas are copies of shards -> provide high availability and fault tolerance if a node fails\n",
    "    },\n",
    "    # mapping defines how documents and fields are indexed and stored, specifying the data type of each field\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} # add course as a keyword field for filtering\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-questions\"\n",
    "\n",
    "es_client = Elasticsearch('http://localhost:9200')\n",
    "es_client.indices.create(index=index_name, body=index_settings)\n",
    "\n",
    "\n",
    "# Index the documents\n",
    "for doc in tqdm(documents):\n",
    "\n",
    "    es_client.index(\n",
    "        index    = index_name, # = where the document will be stored\n",
    "        document = doc\n",
    "    )\n",
    "\n",
    "# Check if the documents are indexed\n",
    "pprint(\n",
    "    es_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***`Q3. Searching`***\n",
    "---\n",
    "\n",
    "Now let's search in our index. \n",
    "\n",
    "We will execute a query \"How do I execute a command in a running docker container?\". \n",
    "\n",
    "Use only `question` and `text` fields and give `question` a boost of 4, and use `\"type\": \"best_fields\"`.\n",
    "\n",
    "What's the score for the top ranking result?\n",
    "\n",
    "* 94.05\n",
    "* 84.05\n",
    "* 74.05\n",
    "* 64.05\n",
    "\n",
    "Hint: Look at the _score field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Helpers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)\n",
    "def elastic_search(query, course=\"data-engineering-zoomcamp\"):\n",
    "    search_query = {\n",
    "        \"size\": 3,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^4\", \"text\"], # !! boost the question field by a factor of 4\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": course\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "\n",
    "    result_docs = []\n",
    "\n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append({\n",
    "            'score':    hit['_score'],  # ! Include the score\n",
    "            'text':     hit['_source']['text'],\n",
    "            'section':  hit['_source']['section'],\n",
    "            'question': hit['_source']['question'],\n",
    "            'course':   hit['_source']['course']\n",
    "        })\n",
    "\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Execute query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for the top-ranking result is: 75.54128\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I execute a command in a running docker container?\"\n",
    "\n",
    "# Execute query\n",
    "response = elastic_search(query, course=\"data-engineering-zoomcamp\")\n",
    "\n",
    "# Print the top score\n",
    "if response:\n",
    "    top_score = response[0]['score']\n",
    "    print(f\"The score for the top-ranking result is: {top_score}\")\n",
    "else:\n",
    "    print(\"No results found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'PGCLI - running in a Docker container',\n",
      "  'score': 75.54128,\n",
      "  'section': 'Module 1: Docker and Terraform',\n",
      "  'text': 'In case running pgcli  locally causes issues or you do not want to '\n",
      "          'install it locally you can use it running in a Docker container '\n",
      "          'instead.\\n'\n",
      "          'Below the usage with values used in the videos of the course for:\\n'\n",
      "          'network name (docker network)\\n'\n",
      "          'postgres related variables for pgcli\\n'\n",
      "          'Hostname\\n'\n",
      "          'Username\\n'\n",
      "          'Port\\n'\n",
      "          'Database name\\n'\n",
      "          '$ docker run -it --rm --network pg-network '\n",
      "          'ai2ys/dockerized-pgcli:4.0.1\\n'\n",
      "          '175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\\n'\n",
      "          'Password for root:\\n'\n",
      "          'Server: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\\n'\n",
      "          'Version: 4.0.1\\n'\n",
      "          'Home: http://pgcli.com\\n'\n",
      "          'root@pg-database:ny_taxi> \\\\dt\\n'\n",
      "          '+--------+------------------+-------+-------+\\n'\n",
      "          '| Schema | Name             | Type  | Owner |\\n'\n",
      "          '|--------+------------------+-------+-------|\\n'\n",
      "          '| public | yellow_taxi_data | table | root  |\\n'\n",
      "          '+--------+------------------+-------+-------+\\n'\n",
      "          'SELECT 1\\n'\n",
      "          'Time: 0.009s\\n'\n",
      "          'root@pg-database:ny_taxi>'},\n",
      " {'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'How do I check compatibility of local and container Spark '\n",
      "              'versions?',\n",
      "  'score': 43.922554,\n",
      "  'section': 'Module 6: streaming with kafka',\n",
      "  'text': 'You can check the version of your local spark using spark-submit '\n",
      "          '--version. In the build.sh file of the Python folder, make sure '\n",
      "          'that SPARK_VERSION matches your local version. Similarly, make sure '\n",
      "          'the pyspark you pip installed also matches this version.'},\n",
      " {'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'Course - how many Zoomcamps in a year?',\n",
      "  'score': 38.684105,\n",
      "  'section': 'General course-related questions',\n",
      "  'text': 'There are 3 Zoom Camps in a year, as of 2024. However, they are for '\n",
      "          'separate courses:\\n'\n",
      "          'Data-Engineering (Jan - Apr)\\n'\n",
      "          'MLOps (May - Aug)\\n'\n",
      "          'Machine Learning (Sep - Jan)\\n'\n",
      "          \"There's only one Data-Engineering Zoomcamp “live” cohort per year, \"\n",
      "          'for the certification. Same as for the other Zoomcamps.\\n'\n",
      "          'They follow pretty much the same schedule for each cohort per '\n",
      "          'zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of '\n",
      "          'the year. If you’re not interested in the Certificate, you can take '\n",
      "          'any zoom camps at any time, at your own pace, out of sync with any '\n",
      "          '“live” cohort.'},\n",
      " {'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'Should I run docker commands from the windows file system or a '\n",
      "              'file system of a Linux distribution in WSL?',\n",
      "  'score': 38.33403,\n",
      "  'section': 'Module 1: Docker and Terraform',\n",
      "  'text': 'It is recommended by the Docker do\\n'\n",
      "          \"[Windows 10 / 11 Home Edition] If you're running a Home Edition, \"\n",
      "          'you can still make it work with WSL2 (Windows Subsystem for Linux) '\n",
      "          'by following the tutorial here\\n'\n",
      "          'If even after making sure your WSL2 (or Hyper-V) is set up '\n",
      "          'accordingly, Docker remains stuck, you can try the option to Reset '\n",
      "          'to Factory Defaults or do a fresh install.'},\n",
      " {'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'Certificate - Can I follow the course in a self-paced mode and '\n",
      "              'get a certificate?',\n",
      "  'score': 35.94081,\n",
      "  'section': 'General course-related questions',\n",
      "  'text': 'No, you can only get a certificate if you finish the course with a '\n",
      "          \"“live” cohort. We don't award certificates for the self-paced mode. \"\n",
      "          'The reason is you need to peer-review capstone(s) after submitting '\n",
      "          'a project. You can only peer-review projects at the time the course '\n",
      "          'is running.'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***`Q4. Filtering`***\n",
    "---\n",
    "\n",
    "Now let's only limit the questions to `machine-learning-zoomcamp`.\n",
    "\n",
    "Return 3 results. What's the 3rd question returned by the search engine?\n",
    "\n",
    "* How do I debug a docker container?\n",
    "* How do I copy files from a different folder into docker container’s working directory?\n",
    "* How do Lambda container images work?\n",
    "* How can I annotate a graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I copy files from a different folder into docker container’s working directory?\n"
     ]
    }
   ],
   "source": [
    "query  = \"How do I execute a command in a running docker container?\"\n",
    "course = \"machine-learning-zoomcamp\"\n",
    "\n",
    "response = elastic_search(\n",
    "    query=query,\n",
    "    course=course\n",
    ")\n",
    "\n",
    "# display 3rd question:\n",
    "print(response[2]['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***`Q5. Building a prompt`***\n",
    "---\n",
    "\n",
    "Now we're ready to build a prompt to send to an LLM. \n",
    "\n",
    "Take the records returned from Elasticsearch in Q4 and use this template to build the context. Separate context entries by two linebreaks (`\\n\\n`)\n",
    "```python\n",
    "context_template = \"\"\"\n",
    "Q: {question}\n",
    "A: {text}\n",
    "\"\"\".strip()\n",
    "```\n",
    "\n",
    "Now use the context you just created along with the \"How do I execute a command in a running docker container?\" question \n",
    "to construct a prompt using the template below:\n",
    "\n",
    "```\n",
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "```\n",
    "\n",
    "What's the length of the resulting prompt? (use the `len` function)\n",
    "\n",
    "* 962\n",
    "* 1462\n",
    "* 1962\n",
    "* 2462\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt length is: 1462\n"
     ]
    }
   ],
   "source": [
    "def build_prompt(query: str, search_results: list[dict[str, any]]) -> str:\n",
    "    \"\"\"\n",
    "    Build a prompt for the chatbot based on the search results.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"Q: {doc['question']}\\nA: {doc['text']}\\n\\n\"\n",
    "\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# only use the first 3 search results\n",
    "context = response\n",
    "\n",
    "prompt = build_prompt(\n",
    "    query          = query,\n",
    "    search_results = context\n",
    ")\n",
    "\n",
    "# Display the prompt length\n",
    "print(f\"The prompt length is: {len(prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***`Q6. Tokens`***\n",
    "---\n",
    "\n",
    "When we use the OpenAI Platform, we're charged by the number of \n",
    "tokens we send in our prompt and receive in the response.\n",
    "\n",
    "The OpenAI python package uses `tiktoken` for tokenization:\n",
    "\n",
    "```bash\n",
    "pip install tiktoken\n",
    "```\n",
    "\n",
    "Let's calculate the number of tokens in our query: \n",
    "\n",
    "```python\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "```\n",
    "\n",
    "Use the `encode` function. How many tokens does our prompt have?\n",
    "\n",
    "* 122\n",
    "* 222\n",
    "* 322\n",
    "* 422\n",
    "\n",
    "Note: to decode back a token into a word, you can use the `decode_single_token_bytes` function:\n",
    "\n",
    "```python\n",
    "encoding.decode_single_token_bytes(63842)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai.api_key = api_key\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: 322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# calculate tokens in query\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\") # encoding method for specific model\n",
    "\n",
    "# encode the prompt into tokens (list of integers)\n",
    "tokens = encoding.encode(prompt)\n",
    "\n",
    "# answer\n",
    "print(f\"\\nAnswer: {len(tokens)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***`Bonus: generating the answer (ungraded)`***\n",
    "---\n",
    "\n",
    "Let's send the prompt to OpenAI. What's the response?\n",
    "\n",
    "Note: you can replace OpenAI with Ollama. See module 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To execute a command in a running Docker container, you can use the `docker exec` command followed by the container ID or name and the command you want to execute. Here's the syntax:\\n\\n```sh\\ndocker exec -it <container_id_or_name> <command>\\n```\\n\\nFor example, if you wanted to run a shell command within a container, you would replace `<container_id_or_name>` with the actual ID or name of the container, and `<command>` with the command you wish to execute. Here’s a specific example assuming you want to start a bash shell session:\\n\\n```sh\\ndocker exec -it <container_id_or_name> /bin/bash\\n```\\n\\nIf you want to execute a specific command without opening an interactive session:\\n\\n```sh\\ndocker exec -it <container_id_or_name> <command>\\n```\\n\\n Simply replace `<command>` with your desired command.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def llm(prompt):\n",
    "    \"\"\"\n",
    "    Run the prompt through the OpenAI language model and return the response (Inference).\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def rag(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Combined function to search the knowledge base, build the prompt, and generate the answer.\n",
    "    \"\"\"\n",
    "    # retrieve search results from knowledge base\n",
    "    search_results = elastic_search(query)\n",
    "    # build the prompt with the search results, from which the model will generate the answer\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    # inference\n",
    "    return llm(prompt)\n",
    "\n",
    "\n",
    "# apply\n",
    "rag(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***`Bonus: calculating the costs (ungraded)`***\n",
    "---\n",
    "\n",
    "Suppose that on average per request we send 150 tokens and receive back 250 tokens.\n",
    "\n",
    "How much will it cost to run 1000 requests?\n",
    "\n",
    "You can see the prices [here](https://openai.com/api/pricing/)\n",
    "\n",
    "On June 17, the prices for gpt4o are:\n",
    "\n",
    "* Input: $0.005 / 1K tokens\n",
    "* Output: $0.015 / 1K tokens\n",
    "\n",
    "You can redo the calculations with the values you got in Q6 and Q7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: $4.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_per_request_tokens = 150\n",
    "receive_back_tokens        = 250\n",
    "\n",
    "run_n_requests = 1000\n",
    "\n",
    "# costs Input: $0.005 / 1K tokens\n",
    "# costs Output: $0.015 / 1K tokens\n",
    "\n",
    "\n",
    "price_input_150_tokens  = 0.005 / 1000 * 150\n",
    "price_output_250_tokens = 0.015 / 1000 * 250\n",
    "\n",
    "# How much does it cost to process 1000 requests?\n",
    "cost_per_request = price_input_150_tokens + price_output_250_tokens\n",
    "\n",
    "total_cost = cost_per_request * run_n_requests\n",
    "\n",
    "# answer\n",
    "print(f\"\\nAnswer: ${total_cost}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
